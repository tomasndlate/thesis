{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomasndlate/thesis/blob/main/ThesisResearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set-up"
      ],
      "metadata": {
        "id": "BrKy0L-RlwDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required dependencies:"
      ],
      "metadata": {
        "id": "VNMhd9kimvrF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be7e0f7b",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -U typing num2words opencv-python decord transformers av accelerate git+https://github.com/huggingface/transformers@v4.49.0-SmolVLM-2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Login into Hugging Face API (using HF token saved in colab):"
      ],
      "metadata": {
        "id": "XmSl3Nqimiv_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRWfHuzHxxvR"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "token = userdata.get('HF_TOKEN')\n",
        "\n",
        "login(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import SmolVLM2-256M model:"
      ],
      "metadata": {
        "id": "-dABZHbgm30J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30e98bc8",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "\n",
        "model_id = \"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    attn_implementation=\"eager\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sliding Window Strategy - Streaming Inference Logic:"
      ],
      "metadata": {
        "id": "qdEZYwzCXTeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the needed data:"
      ],
      "metadata": {
        "id": "83E0t8obnUZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"/content/drive/MyDrive/ThesisResearch/dmd/test.png\"\n",
        "video_path = \"/content/drive/MyDrive/ThesisResearch/dataset/videos/gA_1_s1_2019-03-08T09;31;15+01;00_rgb_body.mp4\" #\"/content/drive/MyDrive/ThesisResearch/dmd/gA/3/s1/gA_3_s1_2019-03-08T10;27;38+01;00_ir_body.mp4\""
      ],
      "metadata": {
        "id": "cWAu-IChnTy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom predict function:"
      ],
      "metadata": {
        "id": "ZRS7dX6Xob6E"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04754f55"
      },
      "source": [
        "from typing import Literal\n",
        "\n",
        "def predict_with_model(\n",
        "    media_type: Literal[\"video\", \"image\"],\n",
        "    media_path: str,\n",
        "    prompt: str,\n",
        "    model,\n",
        "    processor,\n",
        "    max_new_tokens: int =150\n",
        "    ):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": media_type, \"path\": media_path},\n",
        "                {\"type\": \"text\", \"text\": prompt}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    inputs = processor.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\", torch.bfloat16)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    response = processor.batch_decode(output_ids, skip_special_tokens=True)\n",
        "    return response[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def frame_changed(prev, curr, threshold=5):\n",
        "    diff = cv2.absdiff(prev, curr)\n",
        "    return np.mean(diff) > threshold\n"
      ],
      "metadata": {
        "id": "QOfBP9DccREl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "VIDEO_PATH = video_path\n",
        "FRAME_DIFF_THRESHOLD = 3\n",
        "MAX_BUFFER_FRAMES = 8\n",
        "\n",
        "def frame_changed(prev_gray, curr_gray, threshold):\n",
        "    diff = cv2.absdiff(prev_gray, curr_gray)\n",
        "    print(f\"frame change value {np.mean(diff)}\")\n",
        "    return np.mean(diff) > threshold\n",
        "\n",
        "def main2():\n",
        "    cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(f\"Cannot open video: {VIDEO_PATH}\")\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    fps = fps if fps > 0 else 30\n",
        "    TARGET_FPS = 5\n",
        "    STRIDE = max(1, int(round(fps / TARGET_FPS)))\n",
        "\n",
        "    print(f\"Video FPS={fps:.1f}, checking every {STRIDE} frames\")\n",
        "\n",
        "    frame_idx = 0\n",
        "    prev_gray = None\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_idx % STRIDE != 0:\n",
        "            frame_idx += 1\n",
        "            continue\n",
        "\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        gray = cv2.resize(gray, (160, 90))\n",
        "\n",
        "        if prev_gray is not None:\n",
        "            diff = np.mean(cv2.absdiff(prev_gray, gray))\n",
        "            if diff > FRAME_DIFF_THRESHOLD:\n",
        "                print(f\"[CHANGE] frame={frame_idx}, diff={diff:.2f}\")\n",
        "                ## append to content\n",
        "\n",
        "        prev_gray = gray\n",
        "        frame_idx += 1\n",
        "        ## if content > 10 frames\n",
        "        ## call prediction in here\n",
        "\n",
        "    ## if content still has something call prediction in here\n",
        "    cap.release()\n",
        "    print(\"Done.\")\n",
        "\n",
        "main2()\n",
        "\n",
        "\n",
        "\n",
        "#for frame in frames_chunk:\n",
        "#    messages[0][\"content\"].append({\"type\": \"image\", \"image\": frame})\n",
        "\n",
        "#messages[0][\"content\"].append({\"type\": \"text\", \"text\": prompt})\n",
        "\n",
        "## process and generate\n",
        "#inputs = processor.apply_chat_template(...)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1pAm05fxh1lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 1: Zero-Shot Prompting (Gaze Direction)"
      ],
      "metadata": {
        "id": "H6hYqCGVl5al"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image prediction"
      ],
      "metadata": {
        "id": "mbpXaO4n7HOD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7662bba7"
      },
      "source": [
        "experiment_prompt = \"Where is the person looking to in this image?\"\n",
        "predicted_response = predict_with_model(\"image\", image_path, experiment_prompt, model, processor)\n",
        "print(predicted_response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video prediction"
      ],
      "metadata": {
        "id": "ihezHzmW7QNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 2: One-Shot Prompting (Distraction Detection)"
      ],
      "metadata": {
        "id": "5neQ4Uaxl5_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image prediction"
      ],
      "metadata": {
        "id": "OXPUig7i7SDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_image = image_path\n",
        "inference_image = image_path\n",
        "\n",
        "experiment_prompt = f\"\"\"\n",
        "  -User: You are a driver monitoring system that is responsible for assuring\n",
        "   the driver is driving safely and alert when they are distracted. What is\n",
        "   the state of this driver? {example_image}\n",
        "\n",
        "  -Assistant: This driver is distracted because he is having a phonecall while driving\n",
        "\n",
        "  -User: And how about this driver? {inference_image}\n",
        "\"\"\"\n",
        "\n",
        "predicted_response = predict_with_model(\"image\", image_path, experiment_prompt, model, processor)\n",
        "print(predicted_response)"
      ],
      "metadata": {
        "id": "WSg_xfE_pvgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video prediction"
      ],
      "metadata": {
        "id": "edm0mmby7S4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 3: Structured Output (Code-Format)"
      ],
      "metadata": {
        "id": "jacWtXdgl6Qg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image prediction"
      ],
      "metadata": {
        "id": "nwKB8BZ37U5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-shot + Output formatted:"
      ],
      "metadata": {
        "id": "r6V5H5t5vmxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_image = image_path\n",
        "inference_image = image_path\n",
        "\n",
        "experiment_prompt = f\"\"\"\n",
        "  -User: You are a driver monitoring system that is responsible for\n",
        "   assuring the driver is driving safely and alert when they are distracted.\n",
        "   You need to communicate with the HMI to alert the driver, please provide\n",
        "  the following variables with True or False: Distracted, Talking, Using\n",
        "phone. What is the state of this driver? {example_image}\n",
        "\n",
        "  -Assistant: Distracted = True, Talking = No, Using phone=No\n",
        "\n",
        "  -User: And how about this driver? {inference_image}\n",
        "\"\"\"\n",
        "\n",
        "predicted_response = predict_with_model(\"image\", image_path, experiment_prompt, model, processor)\n",
        "print(predicted_response)"
      ],
      "metadata": {
        "id": "hFCQT0dkvp4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video prediction"
      ],
      "metadata": {
        "id": "LCGXO0sm7WQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation and Challenges"
      ],
      "metadata": {
        "id": "ilwBiUKol6gG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tlde70Rml7Bg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l8oGBpZs8svE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/tomasndlate/thesis/blob/main/ThesisResearch.ipynb",
      "authorship_tag": "ABX9TyPH2ojJKmVw+W/AYciCBk/1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}